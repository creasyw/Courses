The CPU scheduling algorithm described in the last chapter is strongly influenced ­ by memory management policies. At least part of a process must be contained in primary memory to run; the CPU cannot execute a process that exists entirely in secondary memory. However, primary memory is a precious resource that frequently cannot contain all active processes in the system. The memory management subsystem decides which processes should reside (at least partially) in main memory, and manages the parts of the virtual address space of a process that are not core resident. It monitors the amount of available primary memory and may periodically write processes to a secondary memory device called the swap device to provide more space in primary memory. At a later time, the kernel reads the data from the swap device back to main memory.

There are three parts to the description of the swapping algorithm: managing space on th� swap device, swapping processes out of main memory, and swapping processes into main memory.

The swap device is a block device in a configurable section of a disk. When:as the kernel allocates space for files one block at a time, it allocates space on the swap device in groups of contiguous blocks. Space allocated for files is used statically; since it will exist for a long time, the allocation scheme is flexible to reduce the amount of fragmentation and, hence, unallocatable space in the file system. But the allocation of space on the swap device is transitory, depending on the pattern of process scheduling. A process that resides on the swap device will eventually migrate back to main memory, freeing the space it had occupied on the swap device. Since speed is critical and the system can do I/0 faster in one multiblock operation than in several single block operations, the kernel allocates contiguous space on the swap device without regard for fragmentation.

The kernel maintains free space for file systems in a linked list of free blocks, accessible from· the file system super block, but it maintains the free space for the swap device in an in-core table, called a map . Maps, used for other resources besides the swap device (some device drivers, for example ), allow a first-fit allocation of contiguous "blocks" of a resource.

The kernel swaps a process out if it needs space in memory. which may result from any of the following:

1. The fork system call must allocate space for a child process,
2. The brk system call increases the size of a process,
3. A process becomes larger b y the natural growth o f its stack,
4. The kernel wants to free space in memory for processes it had previously swapped out and should now swap in.

When the kernel decides that a process is eligible for swapping from main memory, it decrements the reference count of each region in the process and swap!> the region out if its reference count drops to 0. The kernel allocates space on a swap device and locks the process in memory (for cases 1 - 3) , preventing the swapper from swapping it out while the current swap operation is in progress. The kernel saves the swap address of the region in the region table entry.

The kernel swaps as much data as possible per I/0 operation directly between the swap device and user address space, bypassing the buffer cache. If the hardware cannot transfer multiple pages in one operation, the kernel software must iteratively transfer one page of memory at a time. The exact rate of data transfer and its mechanics therefore depend on the capacities of the disk controller and the implementation of memory management, among other factors.

If a process requires more physical memory than is currently allocated to it, either as a result of user stack growth or invocation of the brk system call and if it needs more memory than is currently available, the kernel does an expansion swap of the process. It reserves enough space on the swap device to contain the memory space of the process, including the newly requested space. Then, it adjusts the address translation mapping of the process to account for the new virtual memory but does not assign physical memory (since none was available). Finally, it swaps the process out in a normal swapping operation, zeroing out the newly allocated space on the swap device. When the kernel later swaps the process into memory, it will allocate physical memory according to the new (augmented size) address translation map. When the process resumes execution, it will have enough memory.

Process 0, the swapper, is the only process that swaps processes into memory from swap devices. At the conclusion of system initialization, the swapper goes into an infinite loop, where its only task is to do process swapping. It attempts to swap processes in from the swap device, and it swaps processes out if it needs space in main memory. The swapper sleeps if there is no work for it to do (for example, if there are no processes to swap in) or if it is unable to do any work (there are no processes eligible to swap out); the kernel periodically wakes it up, as will be seen. The kernel schedules the swapper to execute just as it schedules other processes, albeit at higher priority, but the swapper executes only in kernel mode. The swapper makes no system calls but uses internal kernel functions to do swapping; it is the archetype of all kernel processes.

If the swapper successfully swaps in a process, it searches the set of "ready-to­run but swapped out" processes for others to swap in and repeats the above procedure. One of the following situations eventually arises:

- No "ready-to-run" processes exist on the swap device: The swapper goes to sleep until a process on the swap device wakes up or until the kernel swaps out a process that is "ready to run."
- The swapper finds an eligible process to swap in but the system does not contain enough memory: The swapper attempts to swap another process out and, if successful, restarts the swapping algorithm, searching for a process to swap in.

The swapper chooses processes to swap in based o n the amount of time the processes had been swapped out. Another criterion could have been to swap in the highest-priority process that is ready to run, since such processes deserve a better chance to execute. It has been demonstrated that such a policy results in "slightly" better throughput under heavy system load.

/The algorithm for choosing a process to swap out to make room in memory has more serious flaws/, however. First, the swapper swaps out a process based on its priority, memory-residence time, and nice value. Although it swaps out a process only to make room for a process being swapped in, it may swap out a process that does not provide enough memory for the incoming process. For instance, if the swapper attempts to swap in a process that occupies 1 megabyte of memory and the system contains no free memory, it is futile to swap out a process that occupies only 2K bytes of memory. An alternative strategy would be to swap out groups of processes only if they provide enough memory for the incoming process. Experiments using a PDP 11/23 computer have shown that such a strategy can increase system throughput by about 10 percent under heavy loads.

Second, if the swapper sleeps because it could not find enough memory to swap in a process, it searches again for a process to swap in although it had previously chosen one. The reason is that other swapped processes may have awakened in the meantime and they may be more eligible for swapping in than the previously chosen process. But that is small solace to the original process still trying to be swapped in. In some implementations, the swapper tries to swap out many smaller processes to make room for the big process to be swapped in before searching for another process to swap in; this is the revision.

Third, if the swapper chooses a "ready-to-run" process to swap out, it is possible that the process had not executed since it was previously swapped in. Such thrashing is clearly undesirable.

One final danger is worthy of mention. If the swapper attempts to swap out a process but cannot find space on the swap device, a system deadlock could arise if the following four conditions are met: All processes in main memory are asleep, all "ready-to-run" processes are swapped out, there is no room on the swap device for new processes, and there is no room in main memory for incoming processes. /Interest in fixing problems with the swapper has declined/ in recent years as demand paging algorithms have been implemented for UNIX systems.

*Processes tend to execute instructions in small portions of their text space, such as program loops and frequently called subroutines, and their data references tend to cluster in small subsets of the total data space of the process. This is known as the principle of "locality."* Denning [Denning 68] formalized the notion of the working set of a process, which is the set of pages that the process has referenced in its last n memory references; the number n is called the window of the working set. Because the working set is a fraction of the entire process, more processes may fit simultaneously into main memory than in a swapping system, potentially increasing system throughput because of reduced swapping traffic. When a process addresses a page that is not in its working set, it incurs a page fault; in handling the fault, the kernel updates the working set, reading in pages from a secondary device if necessary.

There are two paging states for a page in memory: The page is aging and is not yet eligible for swapping, or the page is eligible for swapping and is available for reassignment to other virtual pages. The first state indicates that a process recently accessed the page, and the page is therefore in its working set. Some machines set a reference bit when they reference a page, but software methods can be substituted if the hardware does not have this feature. The page stealer turns off the reference bit for such pages but remembers how many examinations have passed since the page was last referenced. The first state thus consists of several sub-states, corresponding to the number of passes the page stealer makes before the page is eligible for swapping. When the number exceeds a threshold value, the kernel puts the page into the second state, ready to be swapped. The maximum period that a page can age before it is eligible to be swapped is implementation dependent, constrained by the number of bits available in the page table entry.

If a process attempts to access a page whose valid bit is not set, it incurs a validity fault and the kernel invokes the validity fault handler. The valid bit is not set for pages outside the virtual address space of a process, nor is it set for pages that are part of the virtual address space but do not currently have a physical page assigned to them. The hardware supplies the kernel with the virtual address that was accessed to cause the memory fault, and the kernel finds the page table entry and disk block descriptor for the page. The kernel locks the region containing the page table entry to prevent race conditions that would occur if the page stealer attempted to swap the page out. If the disk block descriptor has no record of the faulted page, the attempted memory reference is invalid and the kernel sends a "segmentation violation" signal to the offending process.

The validity fault handler concludes by setting the valid bit of the page and clearing the modify bit. It recalculates the process priority, because the process may have slept in the fault handler at a kernel-level priority, giving it an unfair scheduling advantage when returning to user mode. Finally, if returning to user mode, it checks for receipt of any signals that occurred while handling the page fault.

The second kind of memory fault that a process can incur is a protection fault, meaning that the process accessed a valid page but the permission bits associated with the page did not permit access. A process also incurs a protection fault when it attempts to write a page whose copy on write bit was set during the fork system call. The kernel must determine whether permission was denied because the page requires a copy on write or whether something truly illegal happened.

The algorithms for demand paging are most efficient if the hardware sets the reference and modify bits and causes a protection fault when a process writes a page whose copy on write bit is set. However, it is possible to implement the paging algorithms described here if the hardware recognizes only the valid and protection bits. If the valid bit is duplicated by a software-valid bit that indicates whether the page is really valid or not, then the kernel could turn off the hardware valid bit and simulate the setting of the other bits in software.

Although demand paging systems treat memory more flexibly thap. swapping systems, situations can arise where the page stealer and validity fault handler thrash because of a shortage of memory. If the sum of the working sets of all processes is greater than the physical memory on a machine, the fault handler will usually sleep, because it cannot allocate pages for a process. The page stealer will not be able to steal pages fast enough, because all pages are in a working set . System throughput suffers because the kernel spends too much time in overhead, rearranging memory at a frantic pace.

The implementation of demand paging a:Jlows processes to execute even though their entire virtual address space is not loaded in memory; therefore the virtual size of a process can exceed the amount of physical memory available in a system. When the kernel runs low on free pages, the page stealer goes through the active pages of every region, marks pages eligible for stealing if they have aged sufficiently, and eventually copies them to a swap device. When a process addresses a virtual page that is currently swapped out, it incurs a validity fault. The kernel invokes the validity fault handler to assign a new physical page to the region and copies the contents of the virtual page to main memory.

With the implementation of the demand paging algorithm, several features improve system performance. First, the kernel uses the /copy on write/ bit for forking processes, removing the need to make physical copies of pages in most cases. Second, the kernel can demand page contents of an executable file from the file system, eliminating the need for =exec= to read the file into memory immediately. This helps performance because such pages may never be needed during the lifetime of a process, and it eliminates extra thrashing caused if the page stealer were to swap such pages from memory before they are used.
