* Functions

Function application (calling a function by putting a space after it and then typing out the parameters) has the highest precedence of them all. What that means for us is that these two statements are equivalent. Besides, it is a very simple example of a common pattern you will see throughout Haskell - /making basic functions that are obviously correct and then combining them into more complex functions/. This way you also avoid repetition. Functions in Haskell don't have to be in any particular order, so it doesn't matter if you define doubleMe first and then doubleUs or if you do it the other way around.

The difference between Haskell's if statement and if statements in imperative languages is that the else part is mandatory in Haskell. Another thing about the if statement in Haskell is that it is an expression. /An expression is basically a piece of code that returns a value/. Because the else is mandatory, an if statement will always return something and that's why it's an expression.

Note the ' at the end of the function name. That apostrophe doesn't have any special meaning in Haskell's syntax. It's a valid character to use in a function name. We usually use ' to either denote a strict version of a function (one that isn't lazy) or a slightly modified version of a function or a variable.

There are two noteworthy things about a function - the first is that functions can't begin with uppercase letters. The second thing is that when a function doesn't take any parameters, we usually say it's a definition (or a name). Because we can't change what names (and functions) mean once we've defined them.

* List and tuples

In Haskell, lists are a homogenous data structure. It stores several elements of the same type. That means that we can have a list of integers or a list of characters but we can't have a list that has a few integers and then a few characters.

A common task is putting two lists together. *Watch out when repeatedly using the =++= operator on long strings*. This is done by using the =++= operator. For list concatination, Haskell will walk through the whole list at the left side of the =++=. So, /put the longer one at the right will be a good practice/. Or, use the append operation =:= to put the new element at the head of the original list.

Two lists can be compared! (if the stuff they contain can be compared) The comparison performs in a lexicographical order. If you want to get an element out of a list by index, use !!. The indices start at 0. In haskell, =head= and =tail= likes the =car= and =cons= in Racket - the former returns the first item of the list while the latter returns the /list/ without the first item.

In some ways, tuples are like lists â€” they are a way to store several values into a single value. However, there are a few fundamental differences. A list of numbers is a list of numbers. That's its type and it doesn't matter if it has only one number in it or an infinite amount of numbers. Tuples, however, are used when you know exactly how many values you want to combine and its type depends on how many components it has and the types of the components. They are denoted with parentheses and their components are separated by commas. Another key difference is that they don't have to be homogenous. Unlike a list, a tuple can contain a combination of several types.

#+begin_src haskell
ghci> let rightTriangles' = [ (a,b,c) | c <- [1..10], b <- [1..c], a <- [1..b], a^2 + b^2 == c^2, a+b+c == 24]
ghci> rightTriangles'
[(6,8,10)]
#+end_src

- this is just so cool! It has nested loops and conditions in a single list comprehension.

* Types and Typeclasses

Haskell has a static type system. The type of every expression is known at compile time, which leads to safer code. Furthermore, Haskell has type inference. A type is a kind of label that every expression has. It tells us in which category of things that expression fits.

Functions also have types. When writing our own functions, we can choose to give them an explicit type declaration. This is generally considered to be good practice except when writing very short functions.

Int and Integer both stand for the type of integers. The difference is that the latter one is /not bounded/.

Here's a simple function that takes three integers and adds them together:

#+begin_src haskell
addThree :: Int -> Int -> Int -> Int
addThree x y z = x + y + z
#+end_src

This signature makes it possible to partially apply input parameters to a function and use it to generate new functions.

A typeclass can be regarded as an interface that define some behavior. If a type is a part of a typeclass, that means that it supports and implement the behavior the typeclass describes. (it is the exactly what an interface does in Golang - it has contract on the behavior of the data that a function receives or generates.) However, it /feels/ like a wider implication. For example,

#+begin_src haskell
Prelude> :t read
read :: Read a => String -> a
Prelude> read "4" + 0
4
Prelude> read "4"
*** Exception: Prelude.read: no parse
Prelude>
Prelude> read "4" :: Int
4
#+end_src

The signature of =read=  can generate a typeclass, which if there is no other operation (e.g. =+0=) to help the compiler infers its type, the compilation fails unless we tell it explicitly about the type.

Most expressions are such that the compiler can infer what their type is by itself. But sometimes, the compiler doesn't know whether to return a value of type Int or Float for an expression like read "5". To see what the type is, Haskell would have to actually evaluate read "5". But since Haskell is a statically typed language, it has to know all the types before the code is compiled (or in the case of GHCI, evaluated). So we have to tell Haskell: "Hey, this expression should have this type, in case you don't know!".

* Syntax in Functions

The pattern matching in Haskell is in a whole new level and fucking crazy...

#+begin_src haskell
lucky :: (Integral a) => a -> String
lucky 7 = "LUCKY NUMBER SEVEN!"
lucky x = "Sorry, you're out of luck, pal!"
#+end_src

The pattern matching does not include the keyword "break", and will /always exit after executing any one of the branches/. The recommanded practice of writing cases from specific to general in LISP becomes mandatory here, because /it *sequentially* checks all patterns from the first to the last/. Besides, there should always be a "catch-all" pattern at the end of the matching. Without it, it is possible to terminate the program while running when it fails to do the pattern matching.

Another important use of pattern matching is with the list -

#+begin_src haskell
>> x:y:z:zs = [1,2,3,4,5,6,7]
>> x
  1
>> zs
  [4,5,6,7]
#+end_src

Note that if you want to bind to several variables (even if one of them is just =_= and doesn't actually bind at all), we have to surround them in parentheses.

#+begin_src haskell
length' []     = 0
length' (x:xs) = 1 + length' xs
#+end_src

There's also a thing called /as patterns/. Those are a handy way of breaking something up according to a pattern and binding it to names whilst still keeping a reference to the whole thing. You do that by putting a name and an @ in front of a pattern. For instance, the pattern =xs@(x:y:ys)=. This pattern will match exactly the same thing as =x:y:ys= but you can easily get the whole list via =xs= instead of repeating yourself by typing out =x:y:ys= in the function body again.

Whereas patterns are a way of making sure a value conforms to some form and deconstructing it, guards are a way of testing whether some property of a value (or several of them) are true or false. That sounds a lot like an if statement and it's very similar. The thing is that guards are a lot more readable when you have several conditions and they play really nicely with patterns.

Although guards look similar to pattern matching, but they are fundamentally different. Every expression behind the guards will return a boolean result, which in turn dictate whether this branch will be executed. Furthermore, any condition within guards could use pattern matching to disassemble something, the result of the matching is either success or failed.

The names we define in the =where= section of a function are only visible to that function, so we don't have to worry about them polluting the namespace of other functions. Notice that all the names are aligned at a single column. If we don't align them nice and proper, Haskell gets confused because then it doesn't know they're all part of the same block. =where= bindings aren't shared across function bodies of different patterns. If you want several patterns of one function to access some shared name, you have to define it globally. =where= bindings can also be nested. /It's a common idiom to make a function and define some helper function in its =where= clause and then to give those functions helper functions as well, each with its own =where= clause/.

Very similar to =where= bindings are =let= bindings. Where bindings are a syntactic construct that let you bind to variables at the end of a function and the whole function can see them, including all the guards. =Let= bindings let you bind to variables anywhere and are expressions themselves, but are very local, so they don't span across guards. Just like any construct in Haskell that is used to bind values to names, let bindings can be used for pattern matching.

The difference is that =let= bindings are expressions themselves, =where= bindings are just syntactic constructs. That is, /for the sake of pattern matching, let bindings cannot be used cross bars, since they are expressions and are firely local in the scope/. Some people prefer =where= bindings because the names come after the function they're being used in. That way, the function body is closer to its name and type declaration and to some that's more readable. (some people also include me...)

Many imperative languages (C, C++, Java, etc.) have case syntax and if you've ever programmed in them, you probably know what it's about. It's about taking a variable and then executing blocks of code for specific values of that variable and then maybe including a catch-all block of code in case the variable has some value for which we didn't set up a case.

Haskell takes that concept and one-ups it. Like the name implies, case expressions are, well, expressions, much like if else expressions and let bindings. Not only can we evaluate expressions based on the possible cases of the value of a variable, we can also do pattern matching. Taking a variable, pattern matching it, evaluating pieces of code based on its value - /pattern matching on parameters in function definitions is actually just syntactic sugar for case expressions/. (Yes... it is a whole new level..)
* Recursion

Recursion is important to Haskell because unlike imperative languages, you do computations in Haskell by /declaring what something is instead of declaring how you get it/. That's why there are no while loops or for loops in Haskell and instead we many times have to use recursion to declare what something is.

There's a pattern for recursion. Usually you define an edge case and then you define a function that does something between some element and the function applied to the rest. It doesn't matter if it's a list, a tree or any other data structure. So when trying to think of a recursive way to solve a problem, try to think of when a recursive solution doesn't apply and see if you can use that as an edge case, think about identities and think about whether you'll break apart the parameters of the function (for instance, lists are usually broken into a head and a tail via pattern matching) and on which part you'll use the recursive call.

* Higher order functions

Haskell functions can take functions as parameters and return functions as return values. A function that does either of those is called a higher order function. Higher order functions aren't just a part of the Haskell experience, they pretty much are the Haskell experience. It turns out that if you want to define computations by defining what stuff is instead of defining steps that change some state and maybe looping them, higher order functions are indispensable. They're a really powerful way of solving problems and thinking about programs.

Every function in Haskell officially only takes one parameter. If we call a function with too few parameters, we get back a partially applied function, meaning a function that takes as many parameters as we left out. Using partial application (calling functions with too few parameters, if you will) is a neat way to create functions on the fly so we can pass them to another function or to seed them with some data.

The type of =max= is =max :: (Ord a) => a -> a -> a=. That can also be written as =max :: (Ord a) => a -> (a -> a)=. That could be read as: =max= takes an =a= and returns (that's the =->=) =a= function that takes an =a= and returns an =a=. /That's why the return type and the parameters of functions are all simply separated with arrows/.

In functional programming, that pattern is achieved with mapping and filtering. You make a function that takes a value and produces some result. We map that function over a list of values and then we filter the resulting list out for the results that satisfy our search. /Thanks to Haskell's laziness, even if you map something over a list several times and filter it several times, it will only pass over the list once/.

Lambdas are basically anonymous functions that are used because we need some functions only once. Normally, we make a lambda with the sole purpose of passing it to a higher-order function. To make a lambda, we write a \ (because it kind of looks like the greek letter lambda if you squint hard enough) and then we write the parameters, separated by spaces. After that comes a -> and then the function body. We usually surround them by parentheses, because otherwise they extend all the way to the right.

The right fold, =foldr= works in a similar way to the left fold, only the accumulator eats up the values from the right. Also, the left fold's binary function has the accumulator as the first parameter and the current value as the second one (so =\acc x -> ...=), the right fold's binary function has the current value as the first parameter and the accumulator as the second one (so =\x acc -> ...=). It kind of makes sense that the right fold has the accumulator on the right, because it folds from the right side. /One big difference/ is that right folds work on infinite lists, whereas left ones don't! To put it plainly, if you take an infinite list at some point and you fold it up from the right, you'll eventually reach the beginning of the list. However, if you take an infinite list at a point and you try to fold it up from the left, you'll never reach an end!

*Folds can be used to implement any function where you traverse a list once, element by element, and then return something based on that. Whenever you want to traverse a list to return something, chances are you want a fold*. That's why folds are, along with maps and filters, one of the most useful types of functions in functional programming.

Whereas normal function application (putting a space between two things) has a really high precedence, the =$= function has the lowest precedence. Function application with a space is left-associative (so =f a b c= is the same as =((f a) b) c)=), function application with =$= is right-associative.

Apart from getting rid of parentheses, =$= means that function application can be treated just like another function. That way, we can, for instance, map function application over a list of functions (this is sick...). When a =$= is encountered, the expression on its right is applied as the parameter to the function on its left

#+begin_src haskell
ghci> map ($ 3) [(4+), (10*), (^2), sqrt]
[7.0,30.0,9.0,1.7320508075688772]
#+end_src

In Haskell, function composition with the =.= function is defined like so: (this is even sicker)

#+begin_src haskell
(.) :: (b -> c) -> (a -> b) -> a -> c
f . g = \x -> f (g x)
#+end_src

#+begin_src haskell
ghci> map (\x -> negate (abs x)) [5,-3,-6,7,-3,2,-19,24]
[-5,-3,-6,-7,-3,-2,-19,-24]

ghci> map (negate . abs) [5,-3,-6,7,-3,2,-19,24]
[-5,-3,-6,-7,-3,-2,-19,-24]
#+end_src

Many times, a point free style is more readable and concise, because it makes you think about functions and what kind of functions composing them results in instead of thinking about data and how it's shuffled around. You can take simple functions and use composition as glue to form more complex functions. However, many times, writing a function in point free style can be less readable if a function is too complex. That's why making long chains of function composition is discouraged. The prefered style is to use let bindings to give labels to intermediary results or split the problem into sub-problems and then put it together so that the function makes sense to someone reading it instead of just making a huge composition chain.
* Modules

A Haskell module is a collection of related functions, types and typeclasses. A Haskell program is a collection of modules where the main module loads up the other modules and then uses the functions defined in them to do something. Having code split up into several modules has quite a lot of advantages. If a module is generic enough, the functions it exports can be used in a multitude of different programs. If your own code is separated into self-contained modules which don't rely on each other too much (we also say they are loosely coupled), you can reuse them later on. It makes the whole deal of writing code more manageable by having it split into several parts, each of which has some sort of purpose.

When you do import =Data.List=, all the functions that Data.List exports become available in the global namespace, meaning that you can call them from wherever in the script. To avoid name clashes, one can

1. Only import the functions that are needed =import Data.List (nub, sort)=
2. Exclude the functions that are not necessary or have clashed names =import Data.List hiding (nub)=
3. Require the import path in the script =import qualified Data.Map=, or =import qualified Data.Map as M=.

The last option makes it so that if we want to reference Data.Map's filter function, we have to do Data.Map.filter, whereas just filter still refers to the normal filter we all know and love, or to reference =Data.Map= 's =filter= function, we just use =M.filter=. (I'd prefer the 3rd option, but not so much for name clashes. It is rather for readability and maintainability - it is much easier to identify all functions that relates to one specific import.)

A great way to pick up new Haskell knowledge is to just click through the standard library reference and explore the modules and their functions. You can also view the Haskell source code for each module. Reading the source code of some modules is a really good way to learn Haskell and get a solid feel for it. -- To search for functions or to find out where they're located, use Hoogle.

Almost every programming language enables you to split your code up into several files and Haskell is no different. When making programs, it's good practice to take functions and types that work towards a similar purpose and put them in a module. That way, you can easily reuse those functions in other programs by just importing your module.

At the beginning of a module, we specify the module name. Then, we specify the functions that it exports and after that, we can start writing the functions. (It is like defining the interface up from)

#+begin_src haskell
module Geometry
( sphereVolume
, sphereArea
, cubeVolume
, cubeArea
, cuboidArea
, cuboidVolume
) where
#+end_src

When making a module, we usually export only those functions that act as a sort of interface to our module so that the implementation is hidden. Modules can also be given a hierarchical structures. Each module can have a number of sub-modules and they can have sub-modules of their own.

The next time you find yourself writing a file that's really big and has a lot of functions, try to see which functions serve some common purpose and then see if you can put them in their own module. You'll be able to just import your module the next time you're writing a program that requires some of the same functionality.

** [[http://stackoverflow.com/questions/18808258/what-does-the-just-syntax-mean-in-haskell][Explain =Just=, =Nothing=, and =Maybe=]]

It's actually just a normal type constructor that happens to be defined in the *Prelude*, which is the standard library that is imported automatically into every module.

The definition looks something like this:

#+begin_src haskell
data Maybe a = Just a | Nothing
#+end_src

That declaration defines a type, =Maybe a=, which is parameterized by a type variable =a=, which just means that you can use it with any type in place of =a=.

*** Constructing and Destructing

The type has two constructors, =Just a= and =Nothing=. When a type has multiple constructors, it means that a value of the type must have been constructed with just one of the possible constructors. For this type, a value was either constructed via =Just= or =Nothing=, there are no other (non-error) possibilities.

Since =Nothing= has no parameter type, when it's used as a constructor it names a constant value that is a member of type =Maybe a= for all types =a=. But the =Just= constructor does have a type parameter, which means that when used as a constructor it acts like a function from type =a= to =Maybe a=, i.e. it has the type =a -> Maybe a=

So, the constructors of a type build a value of that type; the other side of things is when you would like to use that value, and that is where pattern matching comes in to play. Unlike functions, constructors can be used in pattern binding expressions, and this is the way in which you can do *case analysis* of values that belong to types with more than one constructor.

In order to use a =Maybe a= value in a pattern match, you need to provide a pattern for each constructor, like so:

#+begin_src haskell
case maybeVal of
        Nothing   -> "There is nothing!"
                Just val  -> "There is a value, and it is " ++
        (show val)
#+end_src

In that case expression, the first pattern would match if the value was =Nothing=, and the second would match if the value was constructed with =Just=.  If the second one matches, it also binds the name =val= to the parameter that was passed to the =Just= constructor when the value you're matching against was constructed.

*** What =Maybe= Means

Maybe you were already familiar with how this worked; there's not really any magic to =Maybe= values, it's just a normal Haskell Algebraic Data Type (ADT). But it's used quite a bit because it effectively "lifts" or extends a type, such as =Integer= from your example, into a new context in which it has an extra value (=Nothing=) that represents a lack of value! The type system then requires that you check for that extra value before it will let you get at the =Integer= that *might* be there. This prevents a remarkable number of bugs.

Many languages today handle this sort of "no-value" value via NULL references. Tony Hoare, an eminent computer scientist (he invented Quicksort and is a Turing Award winner), owns up to this as his "[[http://qconlondon.com/london-2009/presentation/Null+References:+The+Billion+Dollar+Mistake][billion dollar mistake]]". The =Maybe= type is not the only way to fix this, but it has proven to be an effective way to do it.

*** =Maybe= as a Functor

The idea of transforming one type to another one such that operations on the old type can *also* be transformed to work on the new type is the concept behind the Haskell type class called =Functor=, which =Maybe a= has a useful instance of.

=Functor= provides a method called =fmap=, which maps functions that range over values from the base type (such as =Integer=) to functions that range over values from the lifted type (such as =Maybe Integer=). A function transformed with =fmap= to work on a =Maybe= value works like this:

#+begin_src haskell
case maybeVal of
      Nothing  -> Nothing               -- there is nothing, so just
      return Nothing
            Just val -> Just (f val)    -- there is a value, so
      apply the function to it
#+end_src

So if you have a =Maybe Integer= value =m_x= and an =Int -> Int= function =f=, you can do =fmap f m_x= to apply the function =f= directly to the =Maybe Integer= without worrying if it's actually got a value or not. In fact, you could apply a whole chain of lifted =Integer -> Integer= functions to =Maybe Integer= values and only have to worry about explicitly checking for =Nothing= once when you're finished.

*** Maybe as a Monad

I'm not sure how familiar you are with the concept of a =Monad= yet, but you have at least used =IO a= before, and the type signature =IO a= looks remarkably similar to =Maybe a=. Although =IO= is special in that it doesn't expose its constructors to you and can thus only be "run" by the Haskell runtime system, it's still also a =Functor= in addition to being a =Monad=.  In fact, there's an important sense in which a =Monad= is just a special kind of =Functor= with some extra features, but this isn't the place to get into that.

Anyway, Monads like =IO= map types to new types that represent "computations that result in values" and you can lift functions into =Monad= types via a very =fmap=-like function called   =liftM= that turns a regular function into a "computation that results in the value obtained by evaluating the function."

You have probably guessed (if you have read this far) that =Maybe= is also a =Monad=. It represents "computations that could fail to return a value". Just like with the =fmap= example, this lets you do a whole bunch of computations without having to explicitly check for errors after each step. And in fact, the way the =Monad= instance is constructed, a computation on =Maybe= values *stops* as soon as a =Nothing= is encountered, so it's kind of like an immediate abort or a valueless return in the middle of a computation.
* Making our own types and typeclasses

Value constructors are just functions that take the fields as parameters and return a value of some type (like Shape) as a result. So when we choose not to export them, we just prevent the person importing our module from using those functions, but if some other functions that are exported return a type, we can use them to make values of our custom data types.

Not exporting the value constructors of a data types makes them more abstract in such a way that we hide their implementation. Also, whoever uses our module can't pattern match against the value constructors.

#+begin_src haskell
data Person = Person
                { firstName   :: String
                , lastName    :: String
                , age         :: Int
                , height      :: Float
                , phoneNumber :: String
                , flavor      :: String
                }
  deriving (Show)
#+end_src

So instead of just naming the field types one after another and separating them with spaces, we use curly brackets. First we write the name of the field, for instance, firstName and then we write a double colon :: (also called Paamayim Nekudotayim, haha) and then we specify the type. The resulting data type is exactly the same. The main benefit of this is that it creates functions that lookup fields in the data type. By using record syntax to create this data type, Haskell automatically made these functions: firstName, lastName, age, height, phoneNumber and flavor. There's another benefit to using record syntax. When we derive Show for the type, it displays it differently if we use record syntax to define and instantiate the type.

Using type parameters is very beneficial, but only when using them makes sense. Usually we use them when our data type would work regardless of the type of the value it then holds inside it, like with our Maybe a type. If our type acts as some kind of box, it's good to use them.

Another example of a parameterized type that we've already met is Map k v from Data.Map. The k is the type of the keys in a map and the v is the type of the values. This is a good example of where type parameters are very useful. Having maps parameterized enables us to have mappings from any type to any other type, as long as the type of the key is part of the Ord typeclass. If we were defining a mapping type, we could add a typeclass constraint in the data declaration:

#+begin_src haskell
data (Ord k) => Map k v = ...
#+end_src

However, it's a very strong convention in Haskell to never add typeclass constraints in data declarations. Why? Well, because we don't benefit a lot, but we end up writing more class constraints, even when we don't need them. If we put or don't put the Ord k constraint in the data declaration for Map k v, we're going to have to put the constraint into functions that assume the keys in a map can be ordered. But if we don't put the constraint in the data declaration, we don't have to put (Ord k) => in the type declarations of functions that don't care whether the keys can be ordered or not. So don't put type constraints into data declarations even if it seems to make sense, because you'll have to put them into the function type declarations either way.

Once again, it's very important to distinguish between the type constructor and the value constructor. When declaring a data type, the part before the === is the type constructor and the constructors after it (possibly separated by =|= 's) are value constructors. Giving a function a type of =Vector t t t -> Vector t t t -> t= would be wrong, because we have to put types in *type* declaration and the vector type constructor takes only one parameter, whereas the value constructor takes three. Let's play around with our vectors.

/A typeclass is a sort of an interface that defines some behavior. A type can be made an instance of a typeclass if it supports that behavior/. Typeclasses are more like interfaces. We don't make data from typeclasses. Instead, we first make our data type and then we think about what it can act like. If it can act like something that can be equated, we make it an instance of the Eq typeclass. If it can act like something that can be ordered, we make it an instance of the Ord typeclass.

Previously, we mentioned that when writing types, the [Char] and String types are equivalent and interchangeable. That's implemented with type synonyms. Type synonyms don't really do anything per se, they're just about giving some types different names so that they make more sense to someone reading our code and documentation. Giving the String type synonyms is something that Haskell programmers do when they want to convey more information about what strings in their functions should be used as and what they represent.

(The typeclass is mind-boggling complex... it is able to create new data type as well as having abstractions upon data types. Just to make things worse, the data types can also be recursive.)

#+begin_src haskell
import qualified Data.Map as Map

data Either a b = Left a | Right b deriving (Eq, Ord, Read, Show)
data LockerState = Taken | Free deriving (Show, Eq)
type Code = String
type LockerMap = Map.Map Int (LockerState, Code)

-- fuck the `either`!
lockerLookup :: Int -> LockerMap -> Either String Code
lockerLookup lockerNumber map =
    case Map.lookup lockerNumber map of
        Nothing -> Left $ "Locker number " ++ show lockerNumber ++ " doesn't exist!"
        Just (state, code) -> if state /= Taken
                                then Right code
                                else Left $ "Locker " ++ show lockerNumber ++ " is already taken!"
#+end_src
